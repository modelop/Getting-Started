{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching & Processing the '20newsgroups' Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). Let's extract and process a training set correponding to two categories: *'alt.atheism'* and *'sci.space'*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import csv\n",
    "import re  # RegEx\n",
    "\n",
    "categories = ['alt.atheism', 'sci.space']  # Extracting two specific categories\n",
    "label, cnt = 0, 0\n",
    "\n",
    "with open(\"train.csv\", \"w\", newline = '') as outfile:\n",
    "    w = csv.writer(outfile)\n",
    "    w.writerow(['id', 'comment_text', 'label'])  # Each comment will have an ID and a label ('0' or '1')\n",
    "    \n",
    "    for cat in categories:\n",
    "        train = fetch_20newsgroups(subset = 'train', categories =[cat], remove = ('headers', 'footers', 'quotes'))\n",
    "        \n",
    "        for n in range(len(train.data)):\n",
    "            \n",
    "            comment = train.data[n].replace('\\n', \" \")\n",
    "            # Remove URLs from train and test\n",
    "            comment = re.sub(r'http\\S+', '', comment)\n",
    "            # Remove punctuation marks\n",
    "            punctuation = '!\"#$%&()*+-/,:;<=>?@[\\\\]^_`{|}~'\n",
    "            comment = ''.join(ch for ch in comment if ch not in set(punctuation))\n",
    "            # Convert text to lowercase\n",
    "            comment = comment.lower()\n",
    "            # Remove numbers\n",
    "            comment = re.sub(r'[0-9]+', '', comment)\n",
    "            # Remove whitespaces\n",
    "            comment = ' '.join(comment.split())\n",
    "\n",
    "            if len(comment) < 1000 and len(comment) > 25:  # Discarding very short comments\n",
    "                row = [str(cnt), comment, str(label)]\n",
    "                w.writerow(row)\n",
    "                cnt += 1\n",
    "            elif len(comment) >= 1000:\n",
    "                row = [str(cnt), comment[0:1000], str(label)]  # Limiting large comments to 1000 characters\n",
    "                w.writerow(row)\n",
    "                cnt += 1\n",
    "        label += 1  # Changing the label for the next category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a sample row (last row) from the .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1035', \"many of you at this point have seen a copy of the lunar resources data purchase act by now. this bill also known as the back to the moon bill would authorize the u.s. government to purchase lunar science data from private and nonprofit vendors selected on the basis of competitive bidding with an aggregate cap on bid awards of million. if you have a copy of the bill and can't or don't want to go through all of the legalese contained in all federal legislationdon't both you have a free resource to evaluate the bill for you. your local congressional office listed in the phone bookis staffed by people who can forward a copy of the bill to legal experts. simply ask them to do so and to consider supporting the lunar resources data purchase act. if you do get feedback negative or positive from your congressional office please forward it to david anderman e. yorba linda blvd. apt g fullerton ca or via email to david.andermanofa.fidonet.org. another resource is your local chapter of the nationa\", '1']\n"
     ]
    }
   ],
   "source": [
    "print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now write a corresponding *.json* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "  \n",
    "# Open the CSV\n",
    "f = open('train.csv', 'r')\n",
    "f.seek(0)\n",
    "next(f)\n",
    "reader = csv.DictReader(f, fieldnames = ('id', 'comment_text', 'label'))  \n",
    "\n",
    "# Parse the CSV into JSON  \n",
    "out = json.dumps([ row for row in reader ])  \n",
    "\n",
    "# Save the JSON  \n",
    "f = open('train.json', 'w')\n",
    "f.write(out)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing ELMo Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the ELMo vectors of our training data. Let's begin by loading the training data as a *Pandas* DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At data loading, train is of shape  (1036, 3)  and of type  <class 'pandas.core.frame.DataFrame'>\n",
      "At data loading, train['comment_text'] is of type  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppressing some TensorFlow warnings.\n",
    "\n",
    "from math import floor\n",
    "\n",
    "# Read data\n",
    "train = pd.read_json(\"train.json\", orient = \"records\")\n",
    "\n",
    "print(\"At data loading, train is of shape \", train.shape, \" and of type \", type(train))\n",
    "print(\"At data loading, train['comment_text'] is of type \", type(train['comment_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple of examples in the DataFrame *train*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        comment_text  id  label\n",
      "0  ideologies also split giving more to disagree ...   0      0\n",
      "1  i would rather be at a higher risk of being ki...   1      0\n"
     ]
    }
   ],
   "source": [
    "print(train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load a *spaCy* language model, as well as a trainable *ELMo* model from *TensorFlow Hub*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm  # spaCy's English model. To install: $ python -m spacy download en_core_web_sm\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load spaCy's language model\n",
    "nlp = en_core_web_sm.load(disable = ['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts):\n",
    "    \"\"\"A function to lemmatize texts\"\"\"\n",
    "    output = []\n",
    "    for i in texts:\n",
    "        s = [token.lemma_ for token in nlp(i)]\n",
    "        output.append(' '.join(s))\n",
    "    return output\n",
    "\n",
    "train['comment_text'] = lemmatization(train['comment_text'])\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable = True)\n",
    "\n",
    "def elmo_vectors(x):\n",
    "    \"\"\"A function to compute ELMo embeddings\"\"\"\n",
    "    embeddings = elmo(x.tolist(), signature = \"default\", as_dict = True)[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        #  Return average of ELMo features\n",
    "        return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to extract *ELMo* vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_samples = 1 # set to 1 to train all of input dataset, .5 to train half, etc.\n",
    "\n",
    "batch_size = 50\n",
    "    \n",
    "list_train = [train[i:i+batch_size] for i in range(0,floor(percent_samples*train.shape[0]),batch_size)]\n",
    "\n",
    "# Extract ELMo embeddings - Uncomment the two lines below to compute from scratch; otherwise, load from .pickle file\n",
    "#elmo_train = [elmo_vectors(x['comment_text']) for x in list_train]\n",
    "#elmo_train = np.concatenate(elmo_train, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *ELMo* vectors took a long time to compute. Let's save them in a *.pickle* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save elmo_train\n",
    "pickle_out = open(\"elmo_train.pickle\",\"wb\")\n",
    "pickle.dump(elmo_train, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved ELMo vectors of the training set may be loaded as\n",
    "elmo_train = pickle.load(open(\"elmo_train.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elmo_train is of type:  <class 'numpy.ndarray'> , and of shape:  (1036, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(\"elmo_train is of type: \", type(elmo_train), \", and of shape: \", elmo_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load saved ELMo embeddings from elmo_train.pickle\n",
    "pickle_in = open(\"elmo_train.pickle\", \"rb\")\n",
    "elmo_train_loaded = pickle.load(pickle_in)\n",
    "\n",
    "# Load raw training data to extract labels\n",
    "train = pd.read_json(\"train.json\",orient=\"records\")\n",
    "y_train = train['label']\n",
    "\n",
    "temp = np.array(y_train)\n",
    "y_train = temp.astype(np.int) # Converting labels in y_train to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Setup and Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start tuning the *maximum depth* of the trees first, along with the *min_child_weight*. We set the objective to *‘binary:logistic’* since this is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {'max_depth': [1, 3, 5], 'min_child_weight': [1, 3, 5]}\n",
    "ind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic'}\n",
    "\n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                            cv_params, \n",
    "                            scoring = 'accuracy', cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s run our grid search with 5-fold cross-validation and see which parameters perform the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=0.8, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=1000, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=0, silent=None,\n",
       "                                     subsample=0.8, verbosity=1),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_depth': [1, 3, 5], 'min_child_weight': [1, 3, 5]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.fit(elmo_train_loaded, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our best score and best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9131274131274131"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 1, 'min_child_weight': 3}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try optimizing some other hyperparameters now to see if we can beat a mean of 91.31% accuracy. This time, we will play around with subsampling along with lowering the learning rate to see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {'learning_rate': [0.1, 0.01], 'subsample': [0.7,0.8,0.9]}\n",
    "ind_params = {'n_estimators': 1000, 'seed':0, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth': 1, 'min_child_weight': 3}\n",
    "\n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                            cv_params, \n",
    "                             scoring = 'accuracy', cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=0.8, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=1, min_child_weight=3,\n",
       "                                     missing=None, n_estimators=1000, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=0, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.01],\n",
       "                         'subsample': [0.7, 0.8, 0.9]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.fit(elmo_train_loaded, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check our score and parameters again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9131274131274131"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'subsample': 0.8}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the CV testing performed earlier, we want to utilize the following parameters:\n",
    "\n",
    "* learning_rate = 0.1\n",
    "* Subsample = 0.8\n",
    "* Max_depth = 1\n",
    "* Min_child_weight = 3\n",
    "\n",
    "To increase the performance of *XGBoost*’s speed through many iterations of the training set, and since we are using only *XGBoost*’s API and not *sklearn*’s anymore, we can create a *DMatrix*. This sorts the data initially to optimize for *XGBoost* when it builds trees, making the algorithm more efficient. This is especially helpful when you have a very large number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgdmat = xgb.DMatrix(elmo_train_loaded, y_train) # Creating a DMatrix to make XGBoost more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s specify our parameters and set our stopping criteria. For now, let’s be aggressive with the stopping and say we don’t want the accuracy to improve for at least 100 new trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb optimal parameters\n",
    "our_params = {'eta': 0.1, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth':1, 'min_child_weight':3}\n",
    "\n",
    "cv_xgb = xgb.cv(params = our_params, dtrain = xgdmat, num_boost_round = 3000, nfold = 5,\n",
    "                metrics = ['error'],\n",
    "                early_stopping_rounds = 100) # Look for early stopping that minimizes error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at our CV results to see how accurate we were with these settings. The output is automatically saved into a *Pandas* dataframe for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.084954</td>\n",
       "      <td>0.012887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.083027</td>\n",
       "      <td>0.014848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.083993</td>\n",
       "      <td>0.011766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.084959</td>\n",
       "      <td>0.012539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.083027</td>\n",
       "      <td>0.012824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "444          0.000241         0.000482         0.084954        0.012887\n",
       "445          0.000241         0.000482         0.083027        0.014848\n",
       "446          0.000241         0.000482         0.083993        0.011766\n",
       "447          0.000241         0.000482         0.084959        0.012539\n",
       "448          0.000241         0.000482         0.083027        0.012824"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_xgb.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best iteration: 448**. Our CV test error at this number of iterations is 8.3%, or **91.7% accuracy**.\n",
    "\n",
    "Now that we have our best settings, let’s create this as an *XGBoost* object model that we can reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_params = {'eta': 0.1, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth':1, 'min_child_weight':3}\n",
    "final_gb = xgb.train(our_params, xgdmat, num_boost_round = 448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now save the trained model as a *.pickle* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_gb,open(\"ELMO_nlp_xgboost.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Performance on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us construct a test dataset to analyze the performance of our trained model, and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re  # RegEx\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'sci.space']  # Extracting two specific categories\n",
    "label, cnt = 0, 0\n",
    "\n",
    "with open(\"ELMo_input_data.csv\", \"w\", newline='') as outfile:\n",
    "    w = csv.writer(outfile)\n",
    "    w.writerow(['id', 'comment_text', 'label'])  # Each comment will have an ID and a label ('0' or '1')\n",
    "    \n",
    "    for cat in categories:\n",
    "        test = fetch_20newsgroups(subset = 'test', categories = [cat], remove = ('headers', 'footers', 'quotes'))\n",
    "        \n",
    "        for n in range(len(test.data)):            \n",
    "            \n",
    "            comment = test.data[n].replace('\\n', \" \")            \n",
    "            # Remove URL's from train and test\n",
    "            comment = re.sub(r'http\\S+', '', comment)\n",
    "            # Remove punctuation marks\n",
    "            punctuation = '!\"#$%&()*+-/,:;<=>?@[\\\\]^_`{|}~'\n",
    "            comment = ''.join(ch for ch in comment if ch not in set(punctuation))\n",
    "            # Convert text to lowercase\n",
    "            comment = comment.lower()\n",
    "            # Remove numbers\n",
    "            comment = re.sub(r'[0-9]+', '', comment)\n",
    "            # Remove whitespaces\n",
    "            comment = ' '.join(comment.split())\n",
    "\n",
    "            if len(comment) < 1000 and len(comment) > 25:  # Discarding very short comments\n",
    "                row = [str(cnt), comment, str(label)]\n",
    "                w.writerow(row)\n",
    "                cnt += 1\n",
    "            elif len(comment) >= 1000:\n",
    "                row = [str(cnt), comment[0:1000], str(label)]  # Limiting large comments to 1000 characters\n",
    "                w.writerow(row)\n",
    "                cnt += 1\n",
    "        label += 1  # Changing the label for the next category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "  \n",
    "# Open the CSV\n",
    "f = open('ELMo_input_data.csv', 'r')\n",
    "f.seek(0)\n",
    "next(f)\n",
    "reader = csv.DictReader(f, fieldnames = ('id', 'comment_text', 'label'))  \n",
    "# Parse the CSV into JSON  \n",
    "out = json.dumps([ row for row in reader ])  \n",
    "\n",
    "# Save the JSON  \n",
    "f = open('ELMo_input_data.json', 'w')  \n",
    "f.write(out)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        comment_text  id  label\n",
      "0  some big deletions another in a string of idio...   0      0\n",
      "1  you should wear your nicest boxer shorts and b...   1      0\n",
      "\n",
      "Test data is of shape:  (685, 3)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_json('ELMo_input_data.json', orient = 'records')\n",
    "print(test.head(2))\n",
    "print(\"\\nTest data is of shape: \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate *ELMo* vectors for test data, and save them in a *.pickle* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_samples = 1 # set to 1 to score all of input dataset  \n",
    "batch_size = 50\n",
    "list_test = [test[i:i+batch_size] for i in range(0,floor(percent_samples*test.shape[0]),batch_size)]\n",
    "\n",
    "# Extract ELMo embeddings - Uncomment the two lines below to compute from scartch; otherwise, load from .pcikle file\n",
    "# elmo_vecs = [elmo_vectors(x['comment_text']) for x in list_test]\n",
    "# elmo_vecs = np.concatenate(elmo_vecs, axis = 0)\n",
    "\n",
    "# Save elmo_vecs\n",
    "pickle_out = open(\"elmo_test.pickle\",\"wb\")\n",
    "pickle.dump(elmo_vecs, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we've previously saved elmo_vecs in a .pickle file, we can simply load them\n",
    "pickle_in = open(\"elmo_test.pickle\", \"rb\")\n",
    "elmo_vecs = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the trained *XGBoost* model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"ELMO_nlp_xgboost.pickle\",\"rb\")) # load saved xgboost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate predictions on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = loaded_model.predict(xgb.DMatrix(elmo_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s use *sklearn*’s accuracy metric to see how well we did on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict function for *XGBoost* outputs probabilities by default and not actual class labels. To calculate accuracy we need to convert these to a 0/1 label. We will set 0.5 probability as our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  86.28 %\n"
     ]
    }
   ],
   "source": [
    "y_test = test['label']\n",
    "temp = np.array(y_test)\n",
    "y_test = temp.astype(np.int) # converting y_train to integers\n",
    "\n",
    "print(\"Accuracy on test data: \",round(100*accuracy_score(predictions, y_test),2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying to FastScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we import the *fastscoredeploy* library. This will leverage the FastScore API for deploying assets and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastscoredeploy import ipmagic\n",
    "from fastscore.io import Slot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schemas are going to define the input and output contract of the model execution code with the data transport. We will add one  for input and another output. Schemas leverage the Avro system: https://avro.apache.org/docs/1.8.1/spec.html. The cell magic command **%%schema (name)** at the top defines the name of the schema. \n",
    "\n",
    "**Note**: the name in this command must match the name of the corresponding schema name in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema loaded and bound to three_strings variable\n"
     ]
    }
   ],
   "source": [
    "%%schema three_strings\n",
    "{\n",
    "    \"items\": {\n",
    "        \"fields\": [\n",
    "            {   \"name\": \"label\",        \"type\": \"string\"   },\n",
    "            {   \"name\": \"comment_text\", \"type\": \"string\"   },\n",
    "            {   \"name\": \"id\",           \"type\": \"string\"   }\n",
    "        ],\n",
    "        \"name\": \"three_strings\",\n",
    "        \"type\": \"record\"\n",
    "    },\n",
    "    \"type\": \"array\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema loaded and bound to double variable\n"
     ]
    }
   ],
   "source": [
    "%%schema double\n",
    "{\"type\":\"double\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schemata can also be inferred from sample data using **Schema.infer**. The samples must be given as records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastscoredeploy.Schema import infer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sample = dict({\"id\":\"5\", \"comment_text\":\"Oh my God\", \"label\":\"0\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5', 'comment_text': 'Oh my God', 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"type\": \"record\",\n",
      "    \"name\": \"Rec626896\",\n",
      "    \"fields\": [\n",
      "        {\n",
      "            \"name\": \"id\",\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"comment_text\",\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"label\",\n",
      "            \"type\": \"string\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(infer([input_sample]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may may the inferred schema as a JSON object in a *.avsc* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = json.loads(infer([input_sample]))\n",
    "temp = json.dumps(temp,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"three_strings.avsc\",\"w\")\n",
    "f.write(temp)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to provide the model execution code. This code will be deployed into the engine and used to score new data. \n",
    " - The cell magic command **%%model** at the top defines the name of the model *ELMo_nlp*.\n",
    " - We will not be using *FastScore*'s *call-back* style (begin and action functions), so we use the smar comment **#fastscore.action: unused**\n",
    " - The following smart comments map the schemas to the input and output. The names of the schemas in these smart comments must match the names of the schemas in the cell magic commands above (the name after **%%schema**). \n",
    " - Next, use import statements to pull in the dependencies. Since the engine is containerized, you must include these import statements again even though you included them at the beginning of the notebook. These will need to be added to the Fastscore Engine's Dockerfile and import policy if they are not included in the default engine. \n",
    " - Slot(0) and Slot(1) are the default input and output slots, respectively. Data coming to the input stream will be read and processed as a *Pandas DataFrame*. Any data processing that has to be carried  is done before scoring. The trained model is then loaded (from, say a *.pickle* file), and used to assign predictions to input data. these predictions are assigned to the output slot, Slot(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and bound to ELMo_nlp variable.\n"
     ]
    }
   ],
   "source": [
    "%%model ELMo_nlp\n",
    "\n",
    "#fastscore.action: unused\n",
    "#fastscore.schema.0: three_strings\n",
    "#fastscore.schema.1: double\n",
    "#fastscore.recordsets.1: true\n",
    "#fastscore.module-attached: tensorflow\n",
    "#fastscore.module-attached: tensorflow_hub\n",
    "#fastscore.module-attached: xgboost\n",
    "\n",
    "from fastscore.io import Slot\n",
    "\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "\n",
    "slot0 = Slot(0)  # Input data will be read from Slot(0)\n",
    "slot1 = Slot(1)  # Predictions/scored will be pushed to Slot(1)\n",
    "\n",
    "# input_data will be a Pandas DataFrame\n",
    "#input_data = slot0.read(format = \"pandas.standard\")\n",
    "input_data = slot0.read(format = \"pandas\")  # Use this read method instead if making calls o scoreExplicit()\n",
    "\n",
    "percent_samples = 1 # set to 1 to score all of input dataset\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "list_input_data = []\n",
    "\n",
    "for i in range(0, floor(percent_samples*input_data.shape[0]), batch_size):\n",
    "    list_input_data += [input_data[i:i+batch_size]]\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)\n",
    "\n",
    "globals().update(locals())\n",
    "\n",
    "def elmo_vectors(x):\n",
    "    \"\"\"A function to compute ELMo embeddings\"\"\"\n",
    "    embeddings = elmo(x.tolist(), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        # sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        # sess.run(tf.tables_initializer())\n",
    "        sess.run(tf.compat.v1.tables_initializer())\n",
    "        # return average of ELMo features\n",
    "        return sess.run(tf.reduce_mean(embeddings,1))\n",
    "\n",
    "\n",
    "# Extract ELMo embeddings\n",
    "globals().update(locals())\n",
    "\n",
    "elmo_vecs = [elmo_vectors(x['comment_text']) for x in list_input_data]\n",
    "\n",
    "elmo_vecs = np.concatenate(elmo_vecs, axis = 0)\n",
    "\n",
    "loaded_model = pickle.load(open(\"ELMO_nlp_xgboost.pickle\",\"rb\"))\n",
    "\n",
    "predictions = loaded_model.predict(xgb.DMatrix(elmo_vecs))\n",
    "\n",
    "out = pd.Series(predictions)\n",
    "\n",
    "slot1.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To retrive your model's source code, uncomment the line below\n",
    "# print(ELMo_nlp.source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the model is working as expected, we can pass sample data to the *scoreExplicit* function.\n",
    "\n",
    "**NOTE**: <code>input_data = slot0.read(format=\"pandas.standard\")</code> should be changed to <code>input_data = slot0.read(format=\"pandas\")</code> for *scoreExplicit* to run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input: \n",
      "\n",
      "   comment_text id label\n",
      "0    Oh my God  5     0\n",
      "\n",
      "Sample score:\n",
      "\n",
      "[0    0.854711\n",
      "dtype: float32]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample input: \\n\\n\", pd.DataFrame([input_sample]).head())\n",
    "\n",
    "ELMo_nlp.scoreExplicit(pd.DataFrame([input_sample]))\n",
    "out = Slot(1).output()\n",
    "print(\"\\nSample score:\\n\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to deploy our FastScore-conformed model to a FastScore engine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Connect:  connect\n",
      "Model Manage:  model-manage-1\n",
      "      Engine:  engine-1\n"
     ]
    }
   ],
   "source": [
    "#Let us now deploy the model to FastScore to validate it works within the Engine\n",
    "from fastscoredeploy.suite import Connect\n",
    "\n",
    "connect = Connect(\"https://<url-or-ip>:8000\")\n",
    "print(\"     Connect: \",connect.name)\n",
    "\n",
    "#Then we specify the model-manage to add the model assets to:\n",
    "model_manage = connect.lookup('model-manage')\n",
    "print(\"Model Manage: \",model_manage.name)\n",
    "\n",
    "#And finally we specify the Engine we will deploy to\n",
    "connect.prefer('engine','engine-1')\n",
    "eng = connect.lookup('engine')\n",
    "print(\"      Engine: \", eng.name)\n",
    "\n",
    "# To check current configuration, uncomment the line below\n",
    "# print(\"\\n\",connect.get_config())\n",
    "\n",
    "# To check current fleet info, uncomment the line below\n",
    "# print(\"\\n\",connect.fleet())\n",
    "\n",
    "# To retrieve pneumo messages, uncomment the lines below\n",
    "# pneumo = connect.pneumo.socket()\n",
    "# print(pneumo.recv())\n",
    "# pneumo.close()\n",
    "\n",
    "# To retrieve the Swagger specification of the API supported by the instance, uncomment the lines below\n",
    "# print(connect.get_swagger())\n",
    "\n",
    "# To retrieves version information from the instance, uncomment the line below. A successful reply means instance is healthy.\n",
    "# connect.check_health()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = three_strings.verify(eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     connect is:  <fastscoredeploy.suite.Connect.Connect object at 0x000000002A93F358>\n",
      "        eng  is:  <fastscoredeploy.suite.Engine.Engine object at 0x000000002A93FE10>\n",
      "model_manage is:  <fastscoredeploy.suite.ModelManage.ModelManage object at 0x000000002A91EF98>\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n     connect is: \",connect)\n",
    "print(\"        eng  is: \",  eng)\n",
    "print(\"model_manage is: \",  model_manage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyting looks <span style=\"color:green\"><b>good</b></span>! Let's update Model Manage with our ELMo_nlp model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ELMo_nlp.update(model_manage = model_manage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Models in Model Manage:  ['ELMo_nlp', 'fsrf_classifier']\n",
      "\n",
      "Schemata in Model Manage:  ['double', 'rfinput', 'rfoutput', 'three_strings']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n  Models in Model Manage: \",model_manage.models.names())\n",
    "print(\"\")\n",
    "print(\"Schemata in Model Manage: \",  model_manage.schemata.names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we upload to the model (ELMo_nlp) the trained **XGBoost** model (ELMo_nlp_xgboost), which will be used to give ppredictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastscore.attachment import Attachment\n",
    "\n",
    "att = Attachment('ELMO_nlp_xgboost.tar.gz', datafile='ELMo_nlp_xgboost.tar.gz')\n",
    "att.upload(ELMo_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d22e08459f471fb046663d8debdc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we deploy to the engine. Start by resetting the engine in case it had previously errored.\n",
    "eng.reset()\n",
    "\n",
    "# Deploy! If there are errors, view the container logs for details\n",
    "ELMo_nlp.deploy(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Active model name:  ELMo_nlp\n",
      "Active model type:  python3\n"
     ]
    }
   ],
   "source": [
    "# To ckeck the engine state, uncomment below\n",
    "# print(\"The Engine is: \",eng.state)\n",
    "\n",
    "# To check active model name and type\n",
    "print(\"\\nActive model name: \",eng.active_model.name)\n",
    "print(\"Active model type: \",  eng.active_model.mtype)\n",
    "# print(engine.active_model.jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model attachments:  ['ELMO_nlp_xgboost.tar.gz']\n"
     ]
    }
   ],
   "source": [
    "print(\"Model attachments: \",ELMo_nlp.attachments.names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {
    "c0cd3384c6df4977b68c5a4ad6ee757d": {
     "views": [
      {
       "cell_index": 27
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
