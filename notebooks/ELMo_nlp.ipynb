{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>An ELMo/XGBoost Model for Text Classification</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching  & Processing the '20newsgroups' Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). Let's extract and process a training set correponding to two categories: *'alt.atheism'* and *'sci.space'*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'sci.space'] # Extracting two specific categories\n",
    "idx=0\n",
    "cnt=0\n",
    "with open(\"train.csv\",\"w\",newline='') as outfile:\n",
    "    w = csv.writer(outfile)\n",
    "    w.writerow(['id','comment_text','label']) # each comment will have an id and a label ('0' or '1')\n",
    "    \n",
    "    for cat in categories:\n",
    "        train= fetch_20newsgroups(subset='train', categories=[cat], remove=('headers', 'footers', 'quotes'))\n",
    "        \n",
    "        for n in range(len(train.data)):\n",
    "            \n",
    "            comment = train.data[n].replace('\\n',\" \")\n",
    "            \n",
    "            # remove URL's from train and test\n",
    "            comment = re.sub(r'http\\S+', '', comment)\n",
    "\n",
    "            # remove punctuation marks\n",
    "            punctuation = '!\"#$%&()*+-/,:;<=>?@[\\\\]^_`{|}~'\n",
    "            comment = ''.join(ch for ch in comment if ch not in set(punctuation))\n",
    "\n",
    "            # convert text to lowercase\n",
    "            comment = comment.lower()\n",
    "\n",
    "            # remove numbers\n",
    "            comment = re.sub(r'[0-9]+', '', comment)\n",
    "\n",
    "            # remove whitespaces\n",
    "            comment = ' '.join(comment.split())\n",
    "\n",
    "            if len(comment)<2000 and len(comment)>25: # discarding very short comments and very large comments\n",
    "                cnt=cnt+1\n",
    "                row = [str(cnt), comment, str(idx)]\n",
    "                w.writerow(row)\n",
    "        idx=idx+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a sample row (last row) from the *.csv* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['925', \"many of you at this point have seen a copy of the lunar resources data purchase act by now. this bill also known as the back to the moon bill would authorize the u.s. government to purchase lunar science data from private and nonprofit vendors selected on the basis of competitive bidding with an aggregate cap on bid awards of million. if you have a copy of the bill and can't or don't want to go through all of the legalese contained in all federal legislationdon't both you have a free resource to evaluate the bill for you. your local congressional office listed in the phone bookis staffed by people who can forward a copy of the bill to legal experts. simply ask them to do so and to consider supporting the lunar resources data purchase act. if you do get feedback negative or positive from your congressional office please forward it to david anderman e. yorba linda blvd. apt g fullerton ca or via email to david.andermanofa.fidonet.org. another resource is your local chapter of the national space society. members of the chapter will be happy to work with you to evaluate and support the back to the moon bill. for the address and telephone number of the nearest chapter to you please send email or check the latest issue of ad astra in a library near you. finally if you have requested and not received information about the back to the moon bill please resend your request. the database for the bill was recently corrupted and some information was lost. the authors of the bill thank you for your patience.\", '1']\n"
     ]
    }
   ],
   "source": [
    "print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now write a corresponding *.json* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "  \n",
    "# Open the CSV\n",
    "f = open('train.csv', 'r')\n",
    "reader = csv.DictReader(f, fieldnames = ('id','comment_text','label'))  \n",
    "# Parse the CSV into JSON  \n",
    "out = json.dumps([ row for row in reader ])  \n",
    "\n",
    "# Save the JSON  \n",
    "f = open('train.json', 'w')  \n",
    "f.write(out)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing ELMo Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the ELMo vectors of our training data. Let's begin by loading the training data as a *Pandas* DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At data loading, train is of shape  (925, 3)  and of type  <class 'pandas.core.frame.DataFrame'>\n",
      "At data loading, train['comment_text'] is of type  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from math import floor\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "#read data\n",
    "train = pd.read_json(\"train.json\",orient=\"records\")\n",
    "train = train.iloc[1:] #First record is a repetition of labels - skipping it\n",
    "\n",
    "print(\"At data loading, train is of shape \", train.shape, \" and of type \", type(train))\n",
    "print(\"At data loading, train['comment_text'] is of type \",type(train['comment_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple of examples in the DataFrame *train*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        comment_text id label\n",
      "1  i would rather be at a higher risk of being ki...  1     0\n",
      "2  nope germany has extremely restrictive citizen...  2     0\n"
     ]
    }
   ],
   "source": [
    "print(train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load a *spaCy* language model, as well as a trainable ELMo model from *tensorflow_hub*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm # to install: $ python -m spacy download en_core_web_sm\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Supressing some TensorFlow warnings. Comment this line to display warnings\n",
    "\n",
    "# load spaCy's language model\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'ner'])\n",
    "\n",
    "# function to lemmatize text\n",
    "def lemmatization(texts):\n",
    "    output = []\n",
    "    for i in texts:\n",
    "        s = [token.lemma_ for token in nlp(i)]\n",
    "        output.append(' '.join(s))\n",
    "    return output\n",
    "\n",
    "train['comment_text'] = lemmatization(train['comment_text'])\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "\n",
    "def elmo_vectors(x):\n",
    "    embeddings = elmo(x.tolist(), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        # return average of ELMo features\n",
    "        return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to extract ELMo vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_samples = 1 # set to 1 to train all of input dataset\n",
    "\n",
    "batch_size = 50\n",
    "    \n",
    "list_train = [train[i:i+batch_size] for i in range(0,floor(percent_samples*train.shape[0]),batch_size)]\n",
    "\n",
    "# Extract ELMo embeddings - Uncomment the two lines below to compute from scratch; otherwise, load from .pickle file\n",
    "elmo_train = [elmo_vectors(x['comment_text']) for x in list_train]\n",
    "elmo_train = np.concatenate(elmo_train, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ELMo vectors took a long time to compute. Let's save them in a *.pickle* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save elmo_train\n",
    "pickle_out = open(\"elmo_train.pickle\",\"wb\")\n",
    "pickle.dump(elmo_train, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elmo_train is of type:  <class 'numpy.ndarray'> , and of shape:  (925, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(\"elmo_train is of type: \", type(elmo_train), \", and of shape: \", elmo_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# load elmo_train\n",
    "pickle_in = open(\"elmo_train.pickle\", \"rb\")\n",
    "elmo_train_loaded = pickle.load(pickle_in)\n",
    "\n",
    "#load raw training data to extract labels\n",
    "train = pd.read_json(\"train.json\",orient=\"records\")\n",
    "train = train.iloc[1:]\n",
    "y_train = train['label']\n",
    "\n",
    "temp = np.array(y_train)\n",
    "y_train = temp.astype(np.int) # converting y_train to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model Setup and Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start tuning the maximum depth of the trees first, along with the min_child_weight. We set the objective to *‘binary:logistic’* since this is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\n",
    "ind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic'}\n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                            cv_params, \n",
    "                             scoring = 'accuracy', cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s run our grid search with 5-fold cross-validation and see which parameters perform the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=0.8, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=1000, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=0, silent=None,\n",
       "                                     subsample=0.8, verbosity=1),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_depth': [3, 5, 7], 'min_child_weight': [1, 3, 5]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.fit(elmo_train_loaded, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our best score and best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9156756756756756"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out what were the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'min_child_weight': 3}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try optimizing some other hyperparameters now to see if we can beat a mean of 89.67% accuracy. This time, we will play around with subsampling along with lowering the learning rate to see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {'learning_rate': [0.1, 0.01], 'subsample': [0.7,0.8,0.9]}\n",
    "ind_params = {'n_estimators': 1000, 'seed':0, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth': 5, 'min_child_weight': 3}\n",
    "\n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                            cv_params, \n",
    "                             scoring = 'accuracy', cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=0.8, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=5, min_child_weight=3,\n",
       "                                     missing=None, n_estimators=1000, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=0, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.01],\n",
       "                         'subsample': [0.7, 0.8, 0.9]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.fit(elmo_train_loaded, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check our score and parameters again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9156756756756756"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'subsample': 0.8}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_GBM.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the CV testing performed earlier, we want to utilize the following parameters:\n",
    "\n",
    "-  learning_rate = 0.1\n",
    "-  Subsample = 0.8\n",
    "-  Max_depth = 5\n",
    "-  Min_child_weight = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase the performance of *XGBoost*’s speed through many iterations of the training set, and since we are using only *XGBoost*’s API and not *sklearn*’s anymore, we can create a *DMatrix*. This sorts the data initially to optimize for *XGBoost* when it builds trees, making the algorithm more efficient. This is especially helpful when you have a very large number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgdmat = xgb.DMatrix(elmo_train_loaded, y_train) # Creating a DMatrix to make XGBoost more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s specify our parameters and set our stopping criteria. For now, let’s be aggressive with the stopping and say we don’t want the accuracy to improve for at least 100 new trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb optimal parameters\n",
    "our_params = {'eta': 0.1, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth':5, 'min_child_weight':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xgb = xgb.cv(params = our_params, dtrain = xgdmat, num_boost_round = 3000, nfold = 5,\n",
    "                metrics = ['error'],\n",
    "                early_stopping_rounds = 100) # Look for early stopping that minimizes error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at our CV results to see how accurate we were with these settings. The output is automatically saved into a pandas dataframe for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085405</td>\n",
       "      <td>0.023784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085405</td>\n",
       "      <td>0.023784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085405</td>\n",
       "      <td>0.023784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085405</td>\n",
       "      <td>0.023784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084324</td>\n",
       "      <td>0.022831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "206               0.0              0.0         0.085405        0.023784\n",
       "207               0.0              0.0         0.085405        0.023784\n",
       "208               0.0              0.0         0.085405        0.023784\n",
       "209               0.0              0.0         0.085405        0.023784\n",
       "210               0.0              0.0         0.084324        0.022831"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_xgb.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best iteration: 210. Our CV test error at this number of iterations is 8.4%, or 91.6% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our best settings, let’s create this as an *XGBoost* object model that we can reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_params = {'eta': 0.1, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth':5, 'min_child_weight':3}\n",
    "final_gb = xgb.train(our_params, xgdmat, num_boost_round = 210)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now save the trained model as a *.pickle* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_gb,open(\"ELMO_nlp_xgboost.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Performance on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us construct a test dataset to analyze the performance of our trained model, and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'sci.space'] # Extracting two specific categories\n",
    "idx=0\n",
    "cnt=0\n",
    "with open(\"ELMo_input_data.csv\",\"w\",newline='') as outfile:\n",
    "    w = csv.writer(outfile)\n",
    "    w.writerow(['id','comment_text','label']) # each comment will have an id and a label ('0' or '1')\n",
    "    \n",
    "    for cat in categories:\n",
    "        test= fetch_20newsgroups(subset='test', categories=[cat], remove=('headers', 'footers', 'quotes'))\n",
    "        \n",
    "        for n in range(len(test.data)):            \n",
    "            comment = test.data[n].replace('\\n',\" \")            \n",
    "            # remove URL's from train and test\n",
    "            comment = re.sub(r'http\\S+', '', comment)\n",
    "            # remove punctuation marks\n",
    "            punctuation = '!\"#$%&()*+-/,:;<=>?@[\\\\]^_`{|}~'\n",
    "            comment = ''.join(ch for ch in comment if ch not in set(punctuation))\n",
    "            # convert text to lowercase\n",
    "            comment = comment.lower()\n",
    "            # remove numbers\n",
    "            comment = re.sub(r'[0-9]+', '', comment)\n",
    "            # remove whitespaces\n",
    "            comment = ' '.join(comment.split())\n",
    "\n",
    "            if len(comment)<2000 and len(comment)>25: # discarding very short comments and very large comments\n",
    "                cnt=cnt+1\n",
    "                row = [str(cnt), comment, str(idx)]\n",
    "                w.writerow(row)\n",
    "        idx=idx+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "  \n",
    "# Open the CSV\n",
    "f = open('ELMo_input_data.csv', 'r')\n",
    "reader = csv.DictReader(f, fieldnames = ('id','comment_text','label'))  \n",
    "# Parse the CSV into JSON  \n",
    "out = json.dumps([ row for row in reader ])  \n",
    "\n",
    "# Save the JSON  \n",
    "f = open('ELMo_input_data.json', 'w')  \n",
    "f.write(out)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        comment_text id label\n",
      "1  some big deletions another in a string of idio...  1     0\n",
      "2  you should wear your nicest boxer shorts and b...  2     0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(617, 3)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_json('ELMo_input_data.json',orient='records')\n",
    "test = test.iloc[1:]\n",
    "print(test.head(2))\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate ELMo vectors for test data, and save them in a *.pickle* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_samples = 1 # set to 1 to score all of input dataset  \n",
    "batch_size = 50\n",
    "list_test = [test[i:i+batch_size] for i in range(0,floor(percent_samples*test.shape[0]),batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract ELMo embeddings - Uncomment the two lines below to compute from scartch; otherwise, load from .pcikle file\n",
    "elmo_vecs = [elmo_vectors(x['comment_text']) for x in list_test]\n",
    "elmo_vecs = np.concatenate(elmo_vecs, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save elmo_vecs\n",
    "pickle_out = open(\"elmo_test.pickle\",\"wb\")\n",
    "pickle.dump(elmo_vecs, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the trained XGBoost model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"ELMo_nlp_xgboost.pickle\",\"rb\")) # load saved xgboost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate predictions on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = loaded_model.predict(xgb.DMatrix(elmo_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.87347245e-01 7.31116712e-01 3.28189023e-02 2.58683780e-04\n",
      " 6.39936002e-03 7.10943580e-01 2.15560973e-01 6.60109101e-03\n",
      " 4.70442843e-04 9.96160150e-01 8.58237094e-04 1.93643868e-01\n",
      " 2.04130150e-02 1.43510429e-03 4.26262530e-04 2.59525259e-03\n",
      " 1.42872185e-01 9.98519480e-01 2.19664257e-02 6.64148748e-01\n",
      " 1.38154048e-02 1.53272906e-02 1.21697402e-02 4.27198305e-04\n",
      " 2.21942384e-02 9.52323496e-01 8.99149978e-04 1.43053591e-01\n",
      " 4.64153141e-02 2.97685619e-03 2.06443062e-03 1.40017986e-01\n",
      " 6.57744527e-01 1.47331674e-02 1.39694829e-02 5.53940004e-03\n",
      " 7.73854554e-01 1.45339807e-02 1.32592511e-03 9.90438044e-01\n",
      " 6.48238584e-02 9.98781741e-01 5.94218553e-04 1.05184328e-03\n",
      " 2.77987301e-01 9.10708010e-01 7.50156343e-01 4.10835678e-03\n",
      " 6.78646611e-04 9.36977923e-01 6.52891397e-03 1.23848999e-03\n",
      " 3.05006057e-01 8.78522933e-01 3.61156076e-01 5.01230417e-04\n",
      " 9.31613445e-01 1.56352371e-02 7.54740788e-04 6.94161560e-03\n",
      " 4.38514277e-02 9.81914927e-05 7.04622790e-02 6.76127244e-03\n",
      " 6.34107709e-01 5.34928637e-04 7.50632465e-01 9.07947589e-03\n",
      " 2.59287399e-03 1.92109356e-03 2.90725613e-03 2.16573834e-01\n",
      " 1.30007127e-02 7.84325821e-04 1.42261311e-02 2.82142009e-03\n",
      " 1.05879339e-03 3.15323565e-03 4.29021269e-01 4.76186920e-04\n",
      " 5.96816003e-01 2.34203460e-03 1.38631475e-03 8.01181979e-03\n",
      " 9.67525423e-01 4.48251545e-01 1.75806461e-03 1.64856955e-01\n",
      " 2.21112801e-04 7.56029272e-04 1.09126724e-01 1.25009669e-02\n",
      " 5.74626088e-01 4.34783064e-02 1.49947929e-03 9.47923243e-01\n",
      " 7.15759993e-01 6.98728919e-01 2.55531305e-03 5.94934300e-02\n",
      " 1.91739120e-04 6.22785330e-01 1.37547567e-03 1.27513369e-03\n",
      " 9.62132365e-02 1.93591081e-02 7.68056989e-01 2.75675729e-02\n",
      " 9.96225119e-01 1.38094139e-04 1.84696796e-03 9.01778877e-01\n",
      " 1.47390455e-01 1.20651617e-03 8.64694476e-01 2.86930386e-04\n",
      " 2.77460814e-01 1.82314992e-01 7.19890833e-01 1.80546343e-02\n",
      " 1.97228100e-02 1.02084555e-01 6.46178750e-03 3.34483944e-03\n",
      " 8.51910293e-01 1.64678332e-03 1.87512045e-03 2.56411405e-03\n",
      " 7.72232830e-04 8.78124118e-01 4.01533878e-04 3.62708755e-02\n",
      " 1.19907199e-03 1.08745499e-02 6.16534206e-04 5.31942584e-02\n",
      " 5.16188389e-04 6.68508589e-01 1.42450689e-03 7.51305640e-01\n",
      " 1.45981100e-03 1.42514601e-03 8.44558293e-04 1.16697341e-01\n",
      " 1.23138120e-03 2.70715561e-02 1.75059903e-02 6.14240646e-01\n",
      " 9.95437443e-01 3.02174082e-03 1.15864381e-01 7.62314379e-01\n",
      " 6.31645560e-01 6.73371375e-01 6.48740113e-01 8.72068584e-01\n",
      " 6.81472346e-02 2.87600327e-03 9.06382084e-01 6.93063857e-03\n",
      " 5.41064795e-03 4.72218206e-04 1.21267267e-01 2.04833671e-01\n",
      " 7.01461494e-01 2.47417513e-04 1.78051833e-03 2.65345089e-02\n",
      " 1.98815927e-01 7.91408658e-01 8.17026198e-01 7.66803743e-03\n",
      " 2.29687011e-03 1.60548817e-02 8.03157091e-01 1.58909082e-01\n",
      " 6.95127249e-01 3.72563303e-02 2.40527555e-01 2.32143953e-01\n",
      " 3.64770651e-01 6.80407160e-04 3.81530775e-03 9.23195155e-04\n",
      " 6.28498018e-01 2.23634318e-01 3.96927983e-01 5.40168047e-01\n",
      " 6.35174960e-02 9.11898836e-02 4.60085601e-01 8.03542137e-01\n",
      " 6.51328417e-04 1.53971957e-02 3.09133716e-02 5.56190252e-01\n",
      " 8.09404068e-03 2.36523211e-01 3.28382244e-03 2.21973836e-01\n",
      " 9.49031580e-03 2.65109241e-02 7.47539043e-01 1.55343086e-01\n",
      " 1.29741419e-03 9.88754153e-01 1.18763428e-02 1.65739376e-02\n",
      " 1.35884866e-01 7.08853781e-01 7.74439191e-04 2.44631781e-04\n",
      " 3.57086025e-02 7.99538568e-02 4.08710152e-01 9.75758943e-04\n",
      " 6.63334727e-01 8.75003636e-04 6.82441401e-04 7.68671110e-02\n",
      " 9.66169119e-01 2.84860224e-01 3.01933259e-01 7.82916546e-01\n",
      " 1.18215838e-02 7.31768757e-02 1.39541492e-01 9.65246618e-01\n",
      " 1.38919458e-01 4.54715073e-01 3.46518331e-03 1.65001489e-02\n",
      " 2.19768509e-01 6.57058775e-01 4.89643635e-03 6.63195970e-04\n",
      " 9.34026301e-01 9.03362870e-01 1.14928198e-03 3.74024987e-01\n",
      " 2.18939711e-03 3.34332144e-04 9.24418330e-01 3.85653153e-02\n",
      " 4.37582880e-01 7.41937995e-01 2.31341749e-01 5.39533645e-02\n",
      " 5.43785580e-02 7.79419899e-01 9.78166163e-01 8.09418678e-01\n",
      " 1.73133090e-01 6.65394247e-01 7.52722800e-01 5.61299585e-02\n",
      " 9.22204673e-01 1.24354199e-01 6.23848615e-03 1.32200848e-02\n",
      " 2.32049823e-03 9.69739020e-01 6.27845466e-01 2.14901250e-02\n",
      " 2.09802017e-03 3.77232966e-04 3.36324364e-01 2.03736578e-04\n",
      " 5.38967192e-01 7.96465933e-01 8.68318617e-01 9.99144077e-01\n",
      " 8.59702229e-01 9.99423862e-01 9.99777853e-01 9.98417616e-01\n",
      " 2.20365133e-02 9.99185622e-01 9.99787867e-01 9.99577820e-01\n",
      " 9.99912739e-01 9.97119308e-01 9.99246478e-01 7.65021741e-01\n",
      " 9.99288797e-01 9.94918108e-01 9.99961019e-01 9.99656081e-01\n",
      " 8.57378364e-01 9.79662955e-01 9.85311747e-01 9.96963322e-01\n",
      " 9.96821284e-01 9.99885082e-01 9.99826014e-01 8.68898153e-01\n",
      " 1.45406097e-01 9.99233961e-01 9.99812305e-01 9.91006911e-01\n",
      " 9.64430213e-01 9.96529639e-01 9.99419093e-01 9.99492407e-01\n",
      " 9.98154938e-01 9.99432981e-01 3.54645163e-01 9.98828709e-01\n",
      " 9.96408045e-01 9.92715180e-01 9.60734069e-01 9.99592364e-01\n",
      " 9.99886394e-01 9.99849677e-01 9.99171019e-01 7.99809515e-01\n",
      " 9.88966167e-01 8.25808663e-03 9.37460661e-01 9.98774588e-01\n",
      " 9.84487116e-01 9.96709943e-01 9.86123800e-01 9.99483585e-01\n",
      " 6.76958442e-01 9.98628497e-01 9.99482751e-01 9.99530196e-01\n",
      " 9.26286578e-01 9.95963573e-01 8.82130980e-01 9.99072790e-01\n",
      " 9.97702301e-01 9.99576747e-01 9.99655366e-01 9.93370652e-01\n",
      " 9.89792466e-01 9.62894320e-01 9.97876883e-01 9.99880552e-01\n",
      " 5.10589957e-01 9.93971109e-01 8.74090612e-01 9.99736726e-01\n",
      " 9.99465406e-01 9.89032328e-01 9.96115923e-01 9.92948174e-01\n",
      " 9.99911070e-01 9.25478280e-01 9.88932490e-01 8.66061270e-01\n",
      " 9.98714447e-01 9.14341033e-01 9.99564588e-01 9.69538629e-01\n",
      " 9.57544208e-01 9.99005497e-01 9.99947548e-01 9.84341323e-01\n",
      " 2.66450763e-01 6.11117661e-01 9.99780476e-01 9.13798213e-01\n",
      " 4.86414552e-01 9.98126924e-01 9.98508871e-01 8.11901987e-01\n",
      " 9.98415112e-01 7.00512230e-01 9.95323718e-01 9.98502016e-01\n",
      " 9.99826849e-01 9.63278651e-01 9.00275528e-01 9.98456001e-01\n",
      " 9.99191344e-01 9.23982203e-01 2.60957778e-01 9.91939604e-01\n",
      " 9.99507308e-01 3.68899316e-01 9.94896829e-01 9.78821039e-01\n",
      " 9.95941162e-01 9.89402771e-01 9.94304717e-01 9.99357641e-01\n",
      " 9.81926978e-01 5.83419561e-01 9.96883214e-01 9.93259490e-01\n",
      " 9.99597609e-01 9.98754740e-01 9.73488510e-01 9.97699797e-01\n",
      " 3.29256177e-01 9.96075213e-01 9.91732359e-01 9.97979105e-01\n",
      " 4.01589125e-02 9.91040170e-01 9.99584734e-01 9.99493837e-01\n",
      " 8.51296544e-01 9.97321308e-01 9.99148607e-01 7.92180657e-01\n",
      " 9.98906374e-01 9.99482751e-01 9.94554639e-01 9.91070390e-01\n",
      " 9.99523997e-01 9.98365700e-01 8.85961235e-01 9.60279584e-01\n",
      " 9.99795139e-01 9.95296061e-01 1.76959127e-01 9.26929176e-01\n",
      " 9.58010912e-01 9.97115254e-01 6.81516588e-01 9.83613849e-01\n",
      " 9.99482751e-01 9.67816651e-01 9.04303014e-01 9.98905182e-01\n",
      " 9.59455311e-01 9.79406655e-01 9.93797362e-01 9.94909704e-01\n",
      " 5.36821008e-01 9.94049072e-01 2.08911207e-02 9.99924660e-01\n",
      " 9.99818027e-01 1.95421562e-01 9.99835491e-01 9.97216940e-01\n",
      " 8.76106620e-01 9.99109209e-01 9.87217724e-01 9.98683631e-01\n",
      " 9.89672780e-01 9.99854088e-01 9.99851584e-01 9.48255241e-01\n",
      " 9.99704182e-01 9.98314857e-01 9.97805655e-01 9.91403818e-01\n",
      " 9.99168634e-01 9.99632359e-01 9.99640703e-01 9.53859866e-01\n",
      " 9.72499311e-01 9.95269835e-01 9.59457636e-01 7.26690233e-01\n",
      " 9.99111235e-01 9.49404716e-01 9.09678042e-01 9.97493982e-01\n",
      " 9.93825555e-01 8.86492133e-01 9.99813616e-01 9.84200180e-01\n",
      " 8.77933502e-01 9.92302775e-01 9.99460638e-01 9.98425603e-01\n",
      " 9.99741256e-01 9.79440808e-01 7.14057922e-01 9.97027814e-01\n",
      " 9.98142242e-01 9.99093533e-01 9.74945903e-01 9.30981219e-01\n",
      " 9.99112308e-01 8.73192430e-01 9.99622226e-01 9.57298219e-01\n",
      " 9.89579499e-01 9.76434231e-01 9.98916984e-01 8.92433226e-01\n",
      " 9.98518765e-01 9.99766052e-01 7.88236320e-01 9.15131509e-01\n",
      " 9.99891043e-01 9.99761164e-01 2.57010341e-01 9.99813855e-01\n",
      " 9.95792627e-01 9.97263312e-01 9.97123897e-01 9.99789178e-01\n",
      " 9.99357164e-01 9.99921918e-01 9.50088382e-01 9.99263346e-01\n",
      " 9.75878060e-01 9.85131919e-01 8.63673091e-01 9.99618649e-01\n",
      " 9.99368250e-01 9.99842048e-01 9.61035430e-01 9.99813616e-01\n",
      " 9.99578536e-01 5.04348814e-01 1.01995751e-01 9.98555958e-01\n",
      " 9.99652863e-01 9.98261273e-01 9.97185767e-01 9.99445736e-01\n",
      " 9.98472989e-01 9.99659538e-01 9.97578561e-01 8.58578563e-01\n",
      " 9.96960461e-01 9.99487400e-01 9.99895930e-01 9.84317660e-01\n",
      " 9.92648423e-01 9.62218702e-01 9.95143771e-01 9.99846101e-01\n",
      " 9.76132333e-01 9.95135486e-01 9.99875188e-01 6.11768067e-01\n",
      " 9.99938965e-01 2.08400816e-01 9.88402784e-01 7.54612148e-01\n",
      " 8.75007749e-01 9.99682784e-01 9.99943495e-01 9.99279320e-01\n",
      " 9.98595297e-01 9.95650470e-01 9.46175039e-01 9.99718368e-01\n",
      " 9.25828516e-01 7.23192513e-01 9.56225216e-01 9.14678693e-01\n",
      " 9.99308348e-01 9.99224305e-01 9.96078312e-01 9.99597967e-01\n",
      " 9.99769151e-01 9.99814212e-01 1.80197379e-03 9.93001997e-01\n",
      " 9.99414206e-01 4.64765072e-01 9.99157310e-01 9.99904871e-01\n",
      " 9.75989699e-01 5.47342360e-01 9.99700308e-01 8.63931239e-01\n",
      " 9.90436494e-01 9.98468578e-01 9.98199821e-01 9.99494433e-01\n",
      " 9.98381853e-01 9.67676640e-01 9.98958111e-01 9.89622116e-01\n",
      " 9.99684334e-01 9.91378665e-01 9.99855161e-01 9.99768436e-01\n",
      " 9.85122979e-01 9.85139191e-01 9.99657273e-01 9.87964869e-01\n",
      " 3.18108052e-02 3.73726040e-02 9.99652863e-01 9.99863625e-01\n",
      " 9.99867439e-01 9.98518646e-01 4.15593803e-01 8.82728815e-01\n",
      " 9.97274697e-01 9.96160269e-01 9.88395810e-01 9.99069631e-01\n",
      " 9.99834538e-01 9.79826570e-01 1.81164116e-01 6.01575673e-01\n",
      " 9.99451578e-01 9.97632027e-01 9.77539897e-01 1.04043625e-01\n",
      " 9.81837332e-01 9.98540163e-01 9.99827981e-01 6.99670836e-02\n",
      " 9.99905944e-01 9.99601424e-01 6.21941984e-01 9.99428689e-01\n",
      " 9.98211861e-01 9.53314960e-01 9.84658360e-01 9.99228716e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9.94014204e-01]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s use sklearn’s accuracy metric to see how well we did on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict function for *XGBoost* outputs probabilities by default and not actual class labels. To calculate accuracy we need to convert these to a 0/1 label. We will set 0.5 probability as our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  84.12 %\n"
     ]
    }
   ],
   "source": [
    "y_test = test['label']\n",
    "temp = np.array(y_test)\n",
    "y_test = temp.astype(np.int) # converting y_train to integers\n",
    "\n",
    "print(\"Accuracy on test data: \",round(100*accuracy_score(predictions, y_test),2),\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
